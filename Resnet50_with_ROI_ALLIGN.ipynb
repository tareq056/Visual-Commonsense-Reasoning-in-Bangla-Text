{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Qzp2oRb6BcTnP9uCZWcXYbBiwMTaqdoK",
      "authorship_tag": "ABX9TyO+S7rtNC7syAfYxAeCt7w+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tareq056/Visual-Commonsense-Reasoning-in-Bangla-Text/blob/main/Resnet50_with_ROI_ALLIGN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o9hUDW7LN5qb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "from torchvision.models import resnet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flattener**"
      ],
      "metadata": {
        "id": "zf7H9c-ZzTSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flattener(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Flattens last 3 dimensions to make it only batch size, -1\n",
        "        \"\"\"\n",
        "        super(Flattener, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)"
      ],
      "metadata": {
        "id": "3Vta-9xqOKrR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision --upgrade\n"
      ],
      "metadata": {
        "id": "WCdel3yyOMrv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb7572b-1fa1-479a-c428-7b9dde28e88f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.11.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.ops import roi_align"
      ],
      "metadata": {
        "id": "DdbLVpMlOSSR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.model_zoo as model_zoo"
      ],
      "metadata": {
        "id": "exH55lm5OTc-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxRzm8ubOYo4",
        "outputId": "52c72362-a17d-435e-b83b-6d4a7ce31107"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "USE_IMAGENET_PRETRAINED = True # otherwise use detectron, but that doesnt seem to work?!?"
      ],
      "metadata": {
        "id": "AJHbr9ZtOafB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "VCR_IMAGES_DIR = os.path.join('/content/drive/MyDrive/BERT/VCR', 'vcr1images')"
      ],
      "metadata": {
        "id": "D9hO_ceMOhet"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VCR_IMAGES_DIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mQS51AUdaz2V",
        "outputId": "c004a3b2-c4bf-433c-d57b-81bb3f00d55c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/BERT/VCR/vcr1images'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VCR_ANNOTS_DIR = os.path.join('/content/drive/MyDrive/BERT/VCR', 'vcr1annots')"
      ],
      "metadata": {
        "id": "a7LGFoMDOjBx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "RtyJY2P1Onjy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uAL-tBr0zaEC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pad_Sequence**"
      ],
      "metadata": {
        "id": "w0r2O33WzZW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequence(sequence, lengths):\n",
        "    \"\"\"\n",
        "    :param sequence: [\\sum b, .....] sequence\n",
        "    :param lengths: [b1, b2, b3...] that sum to \\sum b\n",
        "    :return: [len(lengths), maxlen(b), .....] tensor\n",
        "    \"\"\"\n",
        "    output = sequence.new_zeros(len(lengths), max(lengths), *sequence.shape[1:])\n",
        "    start = 0\n",
        "    for i, diff in enumerate(lengths):\n",
        "        if diff > 0:\n",
        "            output[i, :diff] = sequence[start:(start + diff)]\n",
        "        start += diff\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "inpr36Z6OoY_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.hub as hub"
      ],
      "metadata": {
        "id": "2FXtYsKEOsuy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load_Resnet**"
      ],
      "metadata": {
        "id": "-S71JZVFzdVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_resnet(pretrained=True):\n",
        "    # huge thx to https://github.com/ruotianluo/pytorch-faster-rcnn/blob/master/lib/nets/resnet_v1.py\n",
        "    backbone = models.resnet50(pretrained=False)\n",
        "    if pretrained:\n",
        "        state_dict = hub.load_state_dict_from_url(\n",
        "            'https://s3.us-west-2.amazonaws.com/ai2-rowanz/resnet50-e13db6895d81.th',\n",
        "            progress=True\n",
        "        )\n",
        "        backbone.load_state_dict(state_dict)\n",
        "    for i in range(2, 4):\n",
        "        getattr(backbone, 'layer%d' % i)[0].conv1.stride = (2, 2)\n",
        "        getattr(backbone, 'layer%d' % i)[0].conv2.stride = (1, 1)\n",
        "    return backbone\n",
        "\n",
        "_load_resnet()"
      ],
      "metadata": {
        "id": "H9p6Mbf3Ouw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b5d2ebe-db18-4ea0-cf38-2cdee921ce2a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://s3.us-west-2.amazonaws.com/ai2-rowanz/resnet50-e13db6895d81.th\" to /root/.cache/torch/hub/checkpoints/resnet50-e13db6895d81.th\n",
            "100%|██████████| 97.7M/97.7M [00:02<00:00, 36.1MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**load_resnet_imagenet**"
      ],
      "metadata": {
        "id": "RwAvjgiKzhdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_resnet_imagenet(pretrained=True):\n",
        "    # huge thx to https://github.com/ruotianluo/pytorch-faster-rcnn/blob/master/lib/nets/resnet_v1.py\n",
        "    backbone = resnet.resnet50(pretrained=pretrained)\n",
        "    for i in range(2, 4):\n",
        "        getattr(backbone, 'layer%d' % i)[0].conv1.stride = (2, 2)\n",
        "        getattr(backbone, 'layer%d' % i)[0].conv2.stride = (1, 1)\n",
        "    # use stride 1 for the last conv4 layer (same as tf-faster-rcnn)\n",
        "    backbone.layer4[0].conv2.stride = (1, 1)\n",
        "    backbone.layer4[0].downsample[0].stride = (1, 1)\n",
        "\n",
        "    # # Make batchnorm more sensible\n",
        "    # for submodule in backbone.modules():\n",
        "    #     if isinstance(submodule, torch.nn.BatchNorm2d):\n",
        "    #         submodule.momentum = 0.01\n",
        "\n",
        "    return backbone\n",
        "_load_resnet_imagenet()"
      ],
      "metadata": {
        "id": "qsiVMgohOwDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3627d3e-c8eb-49f7-b639-5f085eb61b41"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 156MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Detector**"
      ],
      "metadata": {
        "id": "HOCvHDD2zkZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.ops as ops\n",
        "\n",
        "class SimpleDetector(nn.Module):\n",
        "    def __init__(self, pretrained=True, average_pool=True, semantic=True, final_dim=1024):\n",
        "        \"\"\"\n",
        "        :param average_pool: whether or not to average pool the representations\n",
        "        :param pretrained: Whether we need to load from scratch\n",
        "        :param semantic: Whether or not we want to introduce the mask and the class label early on (default Yes)\n",
        "        \"\"\"\n",
        "        super(SimpleDetector, self).__init__()\n",
        "        # huge thx to https://github.com/ruotianluo/pytorch-faster-rcnn/blob/master/lib/nets/resnet_v1.py\n",
        "        backbone = _load_resnet_imagenet(pretrained=pretrained) if USE_IMAGENET_PRETRAINED else _load_resnet(\n",
        "            pretrained=pretrained)\n",
        "\n",
        "        self.backbone = nn.Sequential(\n",
        "            backbone.conv1,\n",
        "            backbone.bn1,\n",
        "            backbone.relu,\n",
        "            backbone.maxpool,\n",
        "            backbone.layer1,\n",
        "            backbone.layer2,\n",
        "            backbone.layer3,\n",
        "            # backbone.layer4\n",
        "        )\n",
        "        self.roi_align = ops.RoIAlign((7, 7) if USE_IMAGENET_PRETRAINED else (14, 14),\n",
        "                                  spatial_scale=1 / 16, sampling_ratio=0)\n",
        "\n",
        "        if semantic:\n",
        "            self.mask_dims = 32\n",
        "            self.object_embed = torch.nn.Embedding(num_embeddings=81, embedding_dim=128)\n",
        "            self.mask_upsample = torch.nn.Conv2d(1, self.mask_dims, kernel_size=3,\n",
        "                                                  stride=1,\n",
        "                                                  padding=1, bias=True)\n",
        "\n",
        "        else:\n",
        "            self.object_embed = None\n",
        "            self.mask_upsample = None\n",
        "\n",
        "        after_roi_align = [backbone.layer4]\n",
        "        self.final_dim = final_dim\n",
        "        if average_pool:\n",
        "            after_roi_align += [nn.AvgPool2d(7, stride=1), Flattener()]\n",
        "\n",
        "        self.after_roi_align = torch.nn.Sequential(*after_roi_align)\n",
        "        print(\"1211213232142353467568769785653412498776543235687654321458765432\")\n",
        "        self.obj_downsample = torch.nn.Sequential(\n",
        "            torch.nn.Dropout(p=0.1),\n",
        "            torch.nn.Linear(2048 + (128 if semantic else 0), final_dim),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.regularizing_predictor = torch.nn.Linear(2048, 81)\n",
        "\n",
        "    def forward(self,\n",
        "                images: torch.Tensor,\n",
        "                boxes: torch.Tensor,\n",
        "                box_mask: torch.LongTensor,\n",
        "                classes: torch.Tensor = None,\n",
        "                segms: torch.Tensor = None,\n",
        "                ):\n",
        "        \"\"\"\n",
        "        :param images: [batch_size, 3, im_height, im_width]\n",
        "        :param boxes:  [batch_size, max_num_objects, 4] Padded boxes\n",
        "        :param box_mask: [batch_size, max_num_objects] Mask for whether or not each box is OK\n",
        "        :return: object reps [batch_size, max_num_objects, dim]\n",
        "        \"\"\"\n",
        "        # [batch_size, 2048, im_height // 32, im_width // 32\n",
        "       \n",
        "        img_feats = self.backbone(images)\n",
        "        box_inds = box_mask.nonzero()\n",
        "        assert box_inds.shape[0] > 0\n",
        "        rois = torch.cat((\n",
        "            box_inds[:, 0, None].type(boxes.dtype),\n",
        "            boxes[box_inds[:, 0], box_inds[:, 1]],\n",
        "        ), 1)\n",
        "        print(rois)\n",
        "        # Object class and segmentation representations\n",
        "        roi_align_res = self.roi_align(img_feats, rois.float())  # cast rois to float\n",
        "        if self.mask_upsample is not None:\n",
        "            assert segms is not None\n",
        "            segms_indexed = segms[box_inds[:, 0], None, box_inds[:, 1]] - 0.5\n",
        "            print(\"ASASASASASASASSASASASASSASASASASASASASASASSASASASASAS\")\n",
        "            #roi_align_res[:, :self.mask_dims] += self.mask_upsample(segms_indexed)\n",
        "        \n",
        "\n",
        "        post_roialign = self.after_roi_align(roi_align_res)\n",
        "\n",
        "        # Add some regularization, encouraging the model to keep giving decent enough predictions\n",
        "        obj_logits = self.regularizing_predictor(post_roialign)\n",
        "        obj_labels = classes[box_inds[:, 0], box_inds[:, 1]]\n",
        "        cnn_regularization = F.cross_entropy(obj_logits, obj_labels, size_average=True)[None]\n",
        "\n",
        "        feats_to_downsample = post_roialign if self.object_embed is None else torch.cat((post_roialign, self.object_embed(obj_labels)), -1)\n",
        "        roi_aligned_feats = self.obj_downsample(feats_to_downsample)\n",
        "\n",
        "        # Reshape into a padded sequence - this is expensive and annoying but easier to implement and debug...\n",
        "        obj_reps = pad_sequence(roi_aligned_feats, box_mask.sum(1).tolist())\n",
        "        return {\n",
        "            'obj_reps_raw': post_roialign,\n",
        "            'obj_reps': obj_reps,\n",
        "            'obj_logits': obj_logits,\n",
        "            'obj_labels': obj_labels,\n",
        "            'cnn_regularization_loss': cnn_regularization\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "CR2e1RkhO0Je"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision"
      ],
      "metadata": {
        "id": "pyAau-9jQaet"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4N7UBNIQYf7",
        "outputId": "7bc1b84f-8044-49bb-a6db-b743f2e89c09"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.15.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torchvision"
      ],
      "metadata": {
        "id": "0ZcyF28CQkdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9c75a0-5210-4608-ee0f-a4548c6d10a7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.0.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision) (3.11.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (16.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch==2.0.0->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision --upgrade\n"
      ],
      "metadata": {
        "id": "jlOgYr3oR88-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d06910-9b5f-46f7-afd9-58127f52254e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.11.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input data\n",
        "batch_size = 2\n",
        "im_height = 224\n",
        "im_width = 224\n",
        "max_num_objects = 5\n",
        "\n",
        "images = torch.randn(batch_size, 3, im_height, im_width)\n",
        "boxes = torch.randn(batch_size, max_num_objects, 4)\n",
        "box_mask = torch.ones(batch_size, max_num_objects, dtype=torch.long)\n",
        "classes = torch.randint(0, 80, (batch_size, max_num_objects))\n",
        "segms = torch.randn(batch_size, max_num_objects, im_height // 32, im_width // 32)\n"
      ],
      "metadata": {
        "id": "rKqyB6dHP0R8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load** **Image**"
      ],
      "metadata": {
        "id": "TwfHFy_DzqFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load image using Pillow\n",
        "image = Image.open(\"/content/drive/MyDrive/BERT/FOOD IMAGE/archive (1)/evaluation/food/0.jpg\")\n",
        "\n",
        "# Define image transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "# Apply transformation to the image\n",
        "tensor_image = transform(image)\n",
        "\n",
        "# Print the shape of the tensor\n",
        "#print(tensor_image.shape)\n",
        "\n",
        "# Assuming tensor_image has shape (C, H, W)\n",
        "tensor_image_batch1 = tensor_image.unsqueeze(0)  # Add batch dimension of 1\n",
        "tensor_image_batch2 = tensor_image.unsqueeze(0)  # Add batch dimension of 1\n",
        "\n",
        "# Concatenate the two tensors along the batch dimension\n",
        "tensor_image_batch2 = torch.cat([tensor_image_batch1, tensor_image_batch2], dim=0)\n",
        "\n",
        "# Print the shape of the new tensor\n",
        "print(tensor_image_batch2.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdYwoKtNKL0t",
        "outputId": "180e073c-dae9-4a60-a01c-851894c0d0dc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Open the JSONL file for reading\n",
        "with open('/content/drive/MyDrive/BERT/VCR/Images/lsmdc_0001_American_Beauty/0001_American_Beauty_00.00.56.224-00.01.03.394@0.json', 'r') as f:\n",
        "  # Loop through each line in the file\n",
        "  for line in f:\n",
        "    # Load the line as a JSON object\n",
        "    data = json.loads(line)\n",
        "    # Do something with the data\n",
        "    print(data)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Convert the bounding box list to a tensor\n",
        "boxes = tf.constant(data[\"boxes\"], dtype=tf.float32)\n",
        "\n",
        "# Create a boolean mask for valid bounding boxes\n",
        "valid_boxes = tf.reduce_all(boxes > 0, axis=-1)\n",
        "\n",
        "# Convert the boolean mask to integer values\n",
        "valid_boxes = tf.cast(valid_boxes, tf.int32)\n",
        "\n",
        "# Pad the mask with zeros to match the maximum number of objects\n",
        "max_num_objects = 5\n",
        "box_mask = tf.pad(valid_boxes, [[0, max_num_objects - tf.shape(valid_boxes)[0]]])\n",
        "box_mask"
      ],
      "metadata": {
        "id": "ZCnW3DWOzukm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1617cad6-fa41-442a-b19a-da0ce91d2b24"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'boxes': [[966.9033203125, 204.00875854492188, 1899.1513671875, 794.978515625, 0.8180223107337952], [274.30218505859375, 85.06173706054688, 1221.482421875, 818.9290771484375, 0.983723521232605], [5.1060333251953125, 409.7415771484375, 327.98944091796875, 812.7490234375, 0.886288046836853]], 'segms': [[[[1345, 230], [1329, 231], [1314, 232], [1303, 233], [1293, 234], [1284, 235], [1274, 239], [1267, 241], [1263, 242], [1258, 243], [1248, 244], [1244, 245], [1237, 247], [1234, 248], [1231, 249], [1226, 251], [1218, 255], [1215, 257], [1211, 260], [1205, 266], [1202, 270], [1200, 273], [1198, 277], [1196, 283], [1192, 290], [1189, 294], [1179, 304], [1176, 308], [1174, 311], [1170, 319], [1169, 324], [1167, 334], [1166, 340], [1165, 352], [1164, 359], [1163, 362], [1162, 365], [1161, 367], [1158, 372], [1155, 376], [1147, 384], [1145, 387], [1141, 395], [1139, 400], [1138, 403], [1137, 406], [1133, 424], [1132, 431], [1130, 446], [1129, 461], [1126, 482], [1125, 488], [1124, 491], [1115, 507], [1113, 511], [1110, 517], [1108, 522], [1105, 531], [1098, 552], [1097, 557], [1097, 563], [1096, 569], [1095, 574], [1093, 579], [1084, 598], [1083, 600], [1078, 607], [1075, 614], [1070, 619], [1063, 624], [1051, 636], [1046, 643], [1044, 646], [1041, 651], [1038, 658], [1032, 665], [1026, 674], [1025, 676], [1024, 678], [1023, 682], [1022, 687], [1022, 692], [1021, 699], [1018, 705], [1013, 710], [1012, 710], [1006, 716], [1003, 720], [1002, 722], [1001, 725], [1000, 728], [998, 735], [997, 739], [996, 744], [995, 761], [995, 763], [999, 767], [1008, 773], [1016, 777], [1019, 779], [1024, 784], [1026, 785], [1035, 788], [1039, 789], [1043, 790], [1048, 791], [1059, 792], [1074, 793], [1096, 793], [1127, 792], [1158, 791], [1176, 789], [1195, 788], [1214, 787], [1242, 786], [1283, 786], [1299, 788], [1317, 789], [1382, 787], [1392, 787], [1466, 788], [1495, 787], [1515, 785], [1526, 785], [1553, 786], [1575, 784], [1604, 785], [1625, 785], [1674, 784], [1766, 785], [1813, 787], [1817, 787], [1838, 785], [1867, 785], [1868, 784], [1883, 784], [1885, 781], [1887, 777], [1891, 769], [1892, 766], [1893, 763], [1892, 672], [1890, 655], [1890, 632], [1888, 610], [1887, 599], [1886, 593], [1884, 589], [1883, 587], [1877, 586], [1868, 584], [1861, 582], [1852, 579], [1850, 578], [1847, 576], [1842, 571], [1836, 562], [1831, 552], [1821, 542], [1818, 538], [1816, 535], [1815, 533], [1814, 531], [1813, 523], [1813, 513], [1812, 506], [1810, 493], [1809, 487], [1806, 478], [1803, 463], [1800, 445], [1798, 430], [1797, 422], [1796, 413], [1795, 405], [1795, 400], [1796, 384], [1797, 382], [1798, 380], [1808, 370], [1812, 367], [1814, 366], [1816, 365], [1819, 364], [1822, 363], [1825, 363], [1828, 362], [1834, 359], [1840, 357], [1844, 356], [1848, 355], [1854, 354], [1868, 353], [1874, 352], [1875, 351], [1880, 351], [1881, 350], [1883, 350], [1887, 345], [1889, 342], [1890, 340], [1891, 331], [1892, 319], [1889, 315], [1883, 309], [1874, 305], [1871, 303], [1865, 298], [1859, 295], [1850, 292], [1845, 291], [1839, 290], [1826, 288], [1818, 287], [1803, 286], [1784, 285], [1758, 284], [1742, 284], [1717, 286], [1687, 285], [1683, 285], [1655, 287], [1646, 287], [1632, 286], [1619, 285], [1613, 284], [1608, 283], [1604, 282], [1600, 281], [1581, 274], [1574, 272], [1562, 269], [1557, 268], [1552, 267], [1545, 266], [1538, 266], [1531, 265], [1525, 264], [1518, 263], [1514, 262], [1505, 259], [1494, 253], [1485, 250], [1481, 249], [1470, 247], [1457, 245], [1450, 244], [1435, 243], [1428, 242], [1415, 240], [1397, 237], [1392, 236], [1387, 234], [1382, 233], [1373, 232], [1363, 231], [1352, 230]], [[1383, 489], [1385, 491], [1391, 494], [1400, 497], [1405, 498], [1415, 499], [1417, 500], [1422, 504], [1429, 507], [1431, 508], [1432, 510], [1430, 512], [1423, 516], [1421, 517], [1419, 518], [1416, 519], [1412, 519], [1403, 517], [1393, 517], [1382, 516], [1377, 513], [1375, 511], [1374, 511], [1373, 505], [1374, 501], [1375, 498], [1376, 496], [1377, 494]]], [[[722, 85], [697, 86], [692, 87], [688, 88], [684, 89], [681, 90], [678, 91], [673, 93], [665, 97], [663, 99], [655, 103], [650, 105], [641, 108], [621, 112], [617, 113], [610, 115], [607, 116], [599, 119], [597, 120], [595, 121], [592, 123], [589, 126], [586, 128], [581, 131], [579, 132], [577, 133], [570, 136], [562, 138], [556, 141], [552, 144], [547, 149], [544, 155], [540, 160], [535, 165], [531, 168], [527, 170], [521, 175], [519, 178], [517, 182], [513, 190], [510, 198], [507, 204], [505, 207], [495, 217], [491, 219], [485, 225], [483, 228], [482, 230], [480, 237], [477, 249], [476, 254], [475, 262], [473, 279], [472, 291], [471, 305], [471, 309], [472, 323], [475, 349], [475, 387], [472, 393], [470, 396], [467, 400], [461, 406], [460, 406], [455, 411], [446, 423], [444, 426], [442, 429], [438, 437], [436, 440], [430, 446], [425, 449], [415, 459], [413, 462], [412, 464], [411, 467], [410, 471], [408, 484], [407, 488], [406, 491], [405, 494], [404, 496], [403, 498], [400, 503], [396, 508], [387, 517], [385, 521], [382, 527], [380, 532], [379, 535], [378, 538], [377, 542], [376, 547], [374, 566], [373, 571], [372, 574], [370, 579], [367, 586], [364, 591], [362, 594], [356, 600], [353, 604], [351, 607], [349, 610], [348, 612], [347, 614], [344, 621], [343, 627], [342, 634], [341, 648], [340, 654], [339, 659], [338, 664], [337, 668], [336, 672], [335, 675], [332, 681], [330, 684], [327, 688], [321, 694], [319, 697], [317, 700], [316, 702], [315, 704], [313, 709], [309, 721], [307, 728], [305, 741], [304, 745], [302, 752], [301, 755], [300, 758], [298, 763], [294, 771], [290, 776], [288, 780], [289, 782], [290, 784], [293, 788], [305, 803], [305, 804], [307, 806], [310, 808], [312, 809], [314, 810], [319, 812], [322, 813], [340, 814], [360, 814], [373, 813], [385, 812], [396, 811], [407, 810], [417, 809], [426, 808], [431, 807], [435, 806], [446, 802], [458, 799], [465, 799], [473, 800], [480, 801], [492, 804], [501, 805], [512, 806], [529, 807], [554, 808], [563, 808], [570, 807], [577, 806], [591, 802], [618, 803], [637, 803], [660, 801], [663, 801], [669, 802], [685, 806], [692, 807], [732, 807], [751, 806], [794, 802], [810, 801], [827, 800], [876, 800], [891, 798], [929, 798], [940, 799], [957, 802], [966, 803], [972, 803], [990, 802], [1002, 801], [1008, 799], [1015, 797], [1034, 790], [1039, 789], [1065, 785], [1077, 784], [1106, 784], [1133, 782], [1137, 782], [1142, 783], [1152, 785], [1157, 787], [1162, 788], [1167, 789], [1189, 789], [1204, 788], [1206, 786], [1208, 783], [1209, 781], [1210, 779], [1211, 775], [1212, 771], [1213, 766], [1214, 761], [1215, 755], [1215, 710], [1214, 683], [1213, 667], [1210, 645], [1210, 630], [1209, 621], [1208, 617], [1207, 613], [1206, 610], [1204, 606], [1202, 605], [1193, 604], [1183, 604], [1171, 603], [1162, 600], [1157, 598], [1156, 597], [1154, 596], [1166, 584], [1170, 578], [1171, 577], [1177, 573], [1178, 572], [1179, 570], [1177, 568], [1172, 566], [1162, 557], [1158, 554], [1152, 550], [1146, 547], [1143, 545], [1141, 543], [1145, 539], [1148, 535], [1151, 530], [1152, 528], [1153, 526], [1156, 519], [1156, 517], [1154, 516], [1150, 515], [1143, 512], [1139, 510], [1137, 509], [1136, 508], [1136, 507], [1134, 505], [1128, 493], [1128, 489], [1130, 485], [1135, 475], [1143, 467], [1144, 465], [1144, 463], [1138, 454], [1135, 448], [1132, 436], [1128, 427], [1125, 416], [1124, 412], [1122, 409], [1120, 406], [1117, 402], [1110, 394], [1096, 381], [1093, 377], [1091, 374], [1085, 360], [1084, 358], [1082, 355], [1080, 352], [1070, 342], [1065, 339], [1061, 335], [1059, 332], [1057, 328], [1054, 322], [1051, 315], [1050, 310], [1046, 302], [1044, 299], [1040, 295], [1040, 294], [1037, 291], [1028, 283], [1026, 280], [1024, 277], [1021, 271], [1019, 267], [1016, 259], [1013, 253], [1008, 247], [1003, 243], [999, 241], [989, 231], [983, 221], [979, 216], [974, 211], [968, 207], [957, 197], [940, 180], [932, 174], [915, 157], [911, 151], [905, 145], [902, 143], [896, 140], [894, 140], [888, 137], [881, 133], [872, 127], [865, 121], [858, 117], [852, 114], [845, 111], [840, 109], [829, 106], [823, 103], [820, 101], [813, 95], [810, 93], [804, 90], [799, 88], [791, 87], [780, 86], [767, 85]]], [[[21, 419], [15, 420], [14, 421], [11, 421], [10, 423], [9, 425], [8, 428], [7, 432], [6, 443], [6, 760], [7, 788], [8, 796], [9, 801], [10, 805], [12, 807], [16, 807], [22, 809], [55, 809], [101, 808], [170, 808], [175, 808], [190, 808], [201, 808], [202, 809], [213, 808], [221, 808], [279, 809], [285, 808], [288, 807], [290, 806], [291, 803], [293, 800], [295, 795], [296, 792], [297, 787], [298, 778], [298, 775], [297, 758], [296, 748], [295, 745], [294, 742], [292, 737], [290, 733], [286, 731], [264, 732], [255, 731], [232, 727], [222, 726], [212, 726], [204, 725], [198, 724], [194, 723], [190, 721], [184, 718], [174, 708], [172, 705], [171, 703], [168, 694], [167, 690], [166, 683], [166, 676], [162, 656], [159, 647], [158, 642], [157, 635], [156, 614], [154, 606], [154, 598], [153, 590], [152, 583], [150, 577], [149, 570], [149, 563], [146, 551], [145, 543], [145, 533], [142, 518], [142, 510], [141, 502], [140, 497], [137, 488], [136, 482], [135, 476], [134, 468], [134, 459], [133, 452], [132, 445], [131, 442], [128, 436], [123, 430], [122, 429], [118, 426], [116, 425], [114, 424], [110, 423], [105, 422], [95, 421], [49, 421], [24, 419]]]], 'names': ['person', 'person', 'chair'], 'width': 1920, 'height': 822}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([1, 1, 1, 0, 0], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OUTPUT CODE **"
      ],
      "metadata": {
        "id": "FJhTU0GAkEzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# define inputs\n",
        "images = torch.randn(2, 3, 224, 224) # batch of 2 images, each with 3 channels and size 224x224\n",
        "boxes = torch.tensor([[[10, 20, 50, 100], [200, 300, 400, 500], [0, 0, 0, 0]], \n",
        "                      [[50, 60, 100, 200], [0, 0, 0, 0], [0, 0, 0, 0]]]) # boxes for each image with max_num_objects=3\n",
        "box_mask = torch.tensor([[1, 1, 0], [1, 0, 0]]) # mask for each box to indicate if it is valid or not\n",
        "classes = torch.tensor([[0, 1, 2], [3, 0, 0]]) # class labels for each box\n",
        "segms = torch.randn(2, 3, 28, 28) # segmentation masks for each box (optional)\n",
        "\n",
        "# instantiate model\n",
        "model = SimpleDetector(pretrained=True, average_pool=True, semantic=True, final_dim=1024)\n",
        "\n",
        "# run model on inputs\n",
        "object_reps = model(images, boxes, box_mask, classes, segms)\n",
        "\n",
        "# print output shape\n",
        "print(object_reps) # should be [2, 3, 1024]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia2rxvJVLIqt",
        "outputId": "5340006c-8112-4421-c17b-3b5d1c90fb9f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1211213232142353467568769785653412498776543235687654321458765432\n",
            "tensor([[  0,  10,  20,  50, 100],\n",
            "        [  0, 200, 300, 400, 500],\n",
            "        [  1,  50,  60, 100, 200]])\n",
            "ASASASASASASASSASASASASSASASASASASASASASASSASASASASAS\n",
            "{'obj_reps_raw': tensor([[0.3642, 0.4707, 0.8326,  ..., 0.9159, 0.7272, 0.4376],\n",
            "        [0.0023, 0.1574, 0.0000,  ..., 0.0000, 0.0071, 0.0840],\n",
            "        [0.7961, 0.8222, 0.5894,  ..., 0.2019, 0.5385, 0.4440]],\n",
            "       grad_fn=<ViewBackward0>), 'obj_reps': tensor([[[0.0000, 0.1233, 0.3067,  ..., 0.3751, 0.0000, 0.0000],\n",
            "         [0.1490, 0.0000, 0.1786,  ..., 0.2580, 0.2544, 0.0658]],\n",
            "\n",
            "        [[0.1525, 0.4346, 0.0000,  ..., 0.3023, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
            "       grad_fn=<CopySlices>), 'obj_logits': tensor([[-1.7619e-01,  5.9068e-01,  2.2263e-01, -4.3001e-01, -4.4621e-01,\n",
            "          1.5568e-01,  2.6427e-01, -3.2443e-01, -2.7317e-01, -4.2288e-01,\n",
            "          8.1489e-01, -2.4572e-01,  2.2680e-01, -7.7052e-01, -2.5834e-01,\n",
            "          5.5013e-01,  2.4391e-01,  6.8057e-01, -2.3907e-01, -3.7713e-01,\n",
            "         -1.4396e-01, -2.6516e-01,  3.1194e-01, -1.0551e-01, -3.4854e-01,\n",
            "         -1.2094e-01,  7.8619e-01,  1.6855e-02, -3.7642e-01, -1.9831e-01,\n",
            "          6.3585e-01, -2.0568e-01, -8.8064e-02, -5.1529e-01, -4.0215e-01,\n",
            "         -4.2239e-01, -2.9417e-02, -4.0904e-01, -5.2571e-02, -1.8691e-01,\n",
            "         -2.8801e-03,  4.6695e-01,  3.2905e-01,  4.7268e-01, -1.2340e-01,\n",
            "         -1.9748e-01,  7.2049e-01,  1.1186e+00,  4.6131e-02,  7.4423e-01,\n",
            "         -4.1292e-02, -4.4115e-01,  3.4515e-01, -2.5492e-01, -7.4367e-01,\n",
            "         -1.8300e-01, -2.6416e-01, -1.4831e-01,  1.5869e-01, -3.8724e-01,\n",
            "         -1.9060e-01, -3.0124e-02, -3.9408e-01, -2.2754e-01,  3.9235e-01,\n",
            "         -7.7995e-01,  4.7523e-01,  2.8130e-01, -4.9727e-02,  1.6492e-01,\n",
            "          2.1144e-03,  2.3220e-01, -1.6830e-01,  5.2701e-01, -9.0636e-01,\n",
            "          2.6674e-01, -1.5663e-02,  2.4256e-01,  5.0451e-01,  4.7160e-01,\n",
            "          4.1051e-01],\n",
            "        [-6.2898e-02,  7.6628e-02,  1.4136e-01,  2.9779e-02,  1.9878e-01,\n",
            "          2.6338e-01, -8.8438e-02, -9.6019e-02, -2.5253e-01,  4.0350e-02,\n",
            "          1.7380e-01,  3.0596e-02, -1.2713e-01, -8.4863e-02,  2.3629e-01,\n",
            "         -9.8649e-03, -1.2652e-01,  3.4134e-02, -7.6882e-02, -4.2464e-02,\n",
            "         -2.5939e-01,  1.5121e-01,  2.1868e-01,  2.1735e-01, -5.4209e-02,\n",
            "         -1.9992e-01,  2.1389e-01,  7.8429e-02, -2.2128e-01, -1.4754e-01,\n",
            "          1.7202e-01,  9.8289e-02, -7.8502e-02, -3.4757e-01, -1.7509e-01,\n",
            "          5.9431e-02,  1.3981e-01,  4.4497e-02, -8.9152e-02,  3.8152e-01,\n",
            "          1.6306e-01,  1.6820e-01, -1.3660e-01,  3.1624e-01,  1.2715e-01,\n",
            "          1.9047e-02,  1.3529e-01,  3.2969e-01,  5.3532e-02,  4.4203e-01,\n",
            "         -2.7949e-01, -2.4064e-01, -1.5065e-01, -3.5180e-01, -2.4661e-05,\n",
            "         -2.2913e-01,  3.5037e-01, -1.7133e-01,  1.5863e-02,  3.3813e-03,\n",
            "          6.2313e-02, -1.7698e-01,  2.3512e-01, -4.9042e-02,  1.1855e-01,\n",
            "         -1.4815e-01,  2.1063e-01,  1.7033e-01,  1.7204e-01, -1.1507e-01,\n",
            "          1.6561e-01, -1.4802e-01, -4.3710e-02, -2.2408e-02, -2.6616e-01,\n",
            "          2.3020e-01,  1.1314e-01, -1.9743e-01, -3.2960e-02, -1.5987e-02,\n",
            "          1.0215e-01],\n",
            "        [ 7.6120e-03,  1.2079e-01,  3.6282e-01,  5.8507e-02,  3.2161e-01,\n",
            "         -8.3013e-02,  8.1513e-02, -1.7295e-01, -7.4271e-02, -4.8724e-01,\n",
            "          3.6308e-01, -1.9313e-01,  1.3037e-01, -6.2167e-01, -3.9555e-01,\n",
            "          6.3905e-01,  3.4653e-01,  5.4257e-01,  2.3600e-06, -2.6143e-01,\n",
            "         -4.5237e-02,  1.4632e-01,  2.9969e-01, -3.1698e-01, -3.1768e-01,\n",
            "         -5.7852e-02,  5.7600e-01, -5.9803e-02, -3.0986e-01, -1.3139e-01,\n",
            "          2.3313e-01,  1.2992e-01,  4.8209e-01, -3.9734e-01, -4.2131e-01,\n",
            "         -1.8365e-01, -5.7176e-02,  2.6314e-01, -4.8447e-01,  1.7517e-02,\n",
            "          7.9895e-03,  7.7512e-01,  4.9997e-01,  3.7868e-01, -6.8707e-01,\n",
            "         -2.3693e-01,  1.5898e-01,  8.5595e-01,  9.4227e-02,  6.6189e-01,\n",
            "          1.0437e-01, -1.6945e-01,  5.0668e-01, -2.2640e-01, -1.7312e-02,\n",
            "         -5.8253e-02, -2.0887e-01, -5.0969e-01,  1.5658e-02, -4.8174e-01,\n",
            "          2.0686e-02,  1.0293e-01,  5.3834e-02, -1.7713e-01,  2.5113e-01,\n",
            "         -2.0851e-01,  3.0091e-01, -3.7817e-01,  1.8260e-01, -1.8552e-01,\n",
            "         -1.5840e-01,  2.2110e-01, -1.3047e-01,  3.2993e-03, -1.0010e+00,\n",
            "          4.1595e-01,  2.5217e-01,  3.7408e-01,  3.3292e-01,  3.5010e-01,\n",
            "          5.3015e-01]], grad_fn=<AddmmBackward0>), 'obj_labels': tensor([0, 1, 3]), 'cnn_regularization_loss': tensor([4.4811], grad_fn=<UnsqueezeBackward0>)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RUN FORWARD**"
      ],
      "metadata": {
        "id": "F6eVALuK0Eq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an instance of SimpleDetector\n",
        "detector = SimpleDetector()\n",
        "\n",
        "# define input tensors\n",
        "#images = torch.randn((2, 3, 224, 224))  # batch of 2 images with 3 channels, height=224, and width=224\n",
        "images =tensor_image_batch2\n",
        "#boxes = data[\"boxes\"]  # boxes for each image, padded to max_num_objects=2\n",
        "#box_mask = torch.tensor([[1, 1], [1, 0]])  # mask indicating which boxes are valid for each image\n",
        "classes = torch.tensor([[2, 3], [4, 1]])  # object class labels for each box\n",
        "segms = data[\"segms\"]  # object segmentation masks for each box, padded to max_num_objects=2\n",
        "\n",
        "# pass input to the detector module\n",
        "output = detector(images, boxes, box_mask, classes, segms)\n",
        "\n",
        "# print the output\n",
        "print(output)"
      ],
      "metadata": {
        "id": "xx8ezWo00Cs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# create an instance of SimpleDetector\n",
        "detector = SimpleDetector()\n",
        "\n",
        "# define input tensors\n",
        "images = torch.randn((2, 3, 224, 224))  # batch of 2 images with 3 channels, height=224, and width=224\n",
        "#images =tensor_image_batch2\n",
        "boxes = torch.tensor([[[10, 20, 100, 150], [50, 100, 150, 200]], [[5, 10, 80, 120], [30, 60, 100, 160]]])  # boxes for each image, padded to max_num_objects=2\n",
        "box_mask = torch.tensor([[1, 1], [1, 0]])  # mask indicating which boxes are valid for each image\n",
        "classes = torch.tensor([[2, 3], [4, 1]])  # object class labels for each box\n",
        "segms = torch.randn((2, 2, 28, 28))  # object segmentation masks for each box, padded to max_num_objects=2\n",
        "\n",
        "# pass input to the detector module\n",
        "output = detector(images, boxes, box_mask, classes, segms)\n",
        "\n",
        "# print the output\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "heo85nK3B8kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.ops as ops\n",
        "\n",
        "roi_align = ops.RoIAlign(output_size=(7, 7), spatial_scale=1.0, sampling_ratio=-1)"
      ],
      "metadata": {
        "id": "8OMmVYahSW8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roi_align"
      ],
      "metadata": {
        "id": "BXIWGnJdVtj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ILTs6_8--iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# create an instance of SimpleDetector\n",
        "detector = SimpleDetector()\n",
        "\n",
        "\n",
        "batch_size = 2\n",
        "im_height = 224\n",
        "im_width = 224\n",
        "max_num_objects = 5\n",
        "\n",
        "\n",
        "# create demo inputs\n",
        "images = tensor_image_batch2\n",
        "boxes = torch.tensor([[[50, 50, 150, 150], [100, 100, 200, 200]], [[75, 75, 175, 175], [125, 125, 225, 225]]])\n",
        "box_mask = torch.LongTensor([[1, 1], [1, 0]])\n",
        "classes = torch.tensor([[3, 2], [1, 0]])\n",
        "segms = torch.randn(2, 2, 224, 224)\n",
        "\n",
        "# run forward pass\n",
        "detector(images, boxes, box_mask, classes, segms)\n"
      ],
      "metadata": {
        "id": "2l-aKlgtP3o_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}